# RAG 시스템 평가 보고서

## 📋 평가 조건

### 테스트 데이터셋
- **규모**: 50개의 한국어 질문
- **질문 유형**: 
  - 브랜드명 기반 질문 (예: "타이레놀은 어떤 약인가요?")
  - 성분명 기반 질문 (예: "아세트아미노펜이 들어간 약은?")
  - 효능 기반 질문 (예: "두통약 추천해줘")
- **데이터 파일**: [test_dataset.json](file:///C:/Workspaces/SKN22-3rd-1Team/DJAeun/test_dataset.json)

### 평가 환경
- **평가 프레임워크**: Ragas 0.1.0+
- **LLM 모델**: `gpt-4o-mini` (분류 및 답변 생성)
- **임베딩 모델**: `text-embedding-3-small` (관련성 평가)
- **데이터 소스**: OpenFDA API (실시간 검색)
- **검색 제한**: 최대 20개 결과

---

## 🔬 평가 방법

### Ragas 평가 프레임워크
Ragas는 RAG 시스템을 다각도로 평가하는 오픈소스 라이브러리입니다.

### 평가 프로세스
```
1. 질문 입력 → 2. RAG 시스템 답변 생성 → 3. Ragas 평가 → 4. 점수 산출
```

**단계별 상세**:
1. **질문 분류**: LLM이 질문을 분석하여 검색 카테고리 및 키워드 추출
2. **컨텍스트 검색**: OpenFDA API에서 관련 문서 검색
3. **답변 생성**: 검색된 컨텍스트 기반 답변 생성
4. **Ragas 평가**: 4가지 메트릭으로 답변 품질 평가

### 4가지 평가 메트릭

| 메트릭 | 설명 | 측정 대상 |
|--------|------|-----------|
| **Faithfulness** | 답변이 검색된 문서에 얼마나 충실한가 | 답변 ↔ 컨텍스트 |
| **Answer Relevancy** | 답변이 질문과 얼마나 관련 있는가 | 답변 ↔ 질문 |
| **Context Precision** | 검색된 문서가 얼마나 정확한가 | 컨텍스트 품질 |
| **Context Recall** | 필요한 정보를 모두 검색했는가 | 검색 완전성 |

**점수 범위**: 0.0 ~ 1.0 (높을수록 우수)

---

## 📊 평가 내용 (Baseline 결과)

### 전체 점수

```
평균 점수: 0.612 (개선 필요)
```

### 메트릭별 상세 결과

#### ✅ Faithfulness: 0.825 (우수)
**의미**: 답변의 82.5%가 검색된 문서에 근거함

**강점**:
- 검색된 OpenFDA 문서를 잘 활용
- 환각(hallucination) 낮음
- 프롬프트의 "검색된 정보만 사용" 지침 준수

**결론**: 시스템이 문서를 신뢰성 있게 사용하고 있음 👍

---

#### ❌ Answer Relevancy: 0.113 (심각)
**의미**: 답변이 질문과 11.3%만 관련성 있음

**문제점**:
1. **과도한 형식화**
   - 모든 답변이 동일한 마크다운 구조 (### 💊, ⚠️ 등)
   - 간단한 질문에도 복잡한 형식 적용
   
2. **불필요한 정보 과다**
   - "타이레놀 뭐예요?" → 병용금기, 임산부, 저장방법 등 모두 제공
   - 핵심 답변이 희석됨

3. **자연스러운 대화 부족**
   - 형식적이고 기계적인 응답
   - 질문의 의도 파악 미흡

**예시**:
```
질문: "타이레놀은 어떤 약인가요?"
현재: [긴 마크다운 형식의 구조화된 답변]
이상적: "타이레놀은 아세트아미노펜 성분의 해열진통제입니다. 
       두통, 발열, 근육통 등에 사용됩니다."
```

**개선 필요**: 🔴 최우선 과제

---

#### ⚠️ Context Precision: 0.780 (양호)
**의미**: 검색된 문서의 78%가 관련성 있음

**문제점**:
- 22%는 불필요한 정보 포함
- 예: 타이레놀 검색 시 Extra Strength, Regular, 8HR 등 중복 제품 모두 검색

**원인**:
- 동일 성분의 여러 제형이 모두 검색됨
- 필터링 부족

**개선 방법**:
- 중복 제거 (v2_dedup 버전)
- 관련성 기준 재정렬

---

#### ⚠️ Context Recall: 0.730 (양호)
**의미**: 필요한 정보의 73%를 검색함

**문제점**:
- 27%의 중요 정보를 놓침
- 검색 범위 제한 (SEARCH_LIMIT=20)

**원인**:
- 일부 관련 문서가 검색 범위 밖
- 검색 쿼리 최적화 부족

**개선 방법**:
- 두 단계 검색 (v3_twostage 버전)
- 검색 범위 확대 및 재정렬

---

## 💡 개선 방안

### 우선순위별 개선 제안

#### 🔴 1순위: 프롬프트 최적화 (Answer Relevancy 향상)
**목표**: 0.113 → 0.6+

**방법**:
- 질문 유형별 답변 형식 조정
- 간단한 질문 → 간결한 답변
- 상세 요청 → 구조화된 답변
- 불필요한 형식 제거

**예상 효과**: 
- Answer Relevancy 대폭 향상
- 사용자 만족도 증가
- 비용: 무료

---

#### 🟡 2순위: 중복 제거 적용 (Context Precision 향상)
**목표**: 0.780 → 0.85+

**방법**:
- v2_dedup 버전 사용
- 동일 성분 제품 중 대표 1개만 선택
- 노이즈 감소

**예상 효과**:
- 검색 결과 품질 향상
- 답변 생성 시간 단축

---

#### 🟡 3순위: 두 단계 검색 (Context Recall 향상)
**목표**: 0.730 → 0.80+

**방법**:
- v3_twostage 버전 사용
- 1단계: 광범위 검색 (20개)
- 2단계: 관련성 재정렬 후 상위 5개 선택

**예상 효과**:
- 검색 완전성 향상
- 더 관련성 높은 문서 선택

---

#### 🟢 4순위: GPT-4 업그레이드
**목표**: 전반적 품질 향상

**방법**:
- v1_gpt4 버전 사용
- gpt-4o-mini → gpt-4o

**예상 효과**:
- Faithfulness 향상
- Answer Relevancy 향상
- 비용: 15배 증가 (고려 필요)

---

## 🎯 다음 단계

### 즉시 실행 가능한 작업

1. **프롬프트 최적화** (추천)
   - 가장 큰 문제(Answer Relevancy) 해결
   - 비용 무료
   - 빠른 효과

2. **나머지 7개 버전 평가**
   ```bash
   python compare_optimizations.py
   ```
   - 8개 버전 일괄 비교
   - 최적 조합 발견

3. **개별 버전 테스트**
   ```bash
   python evaluate_single.py --config v2_dedup
   python evaluate_single.py --config v3_twostage
   ```

---

## 📁 관련 파일

- [Baseline 평가 결과](file:///C:/Workspaces/SKN22-3rd-1Team/DJAeun/evaluation_baseline.json)
- [테스트 데이터셋](file:///C:/Workspaces/SKN22-3rd-1Team/DJAeun/test_dataset.json)
- [생성된 답변](file:///C:/Workspaces/SKN22-3rd-1Team/DJAeun/generated_answers.json)
- [최적화 설정](file:///C:/Workspaces/SKN22-3rd-1Team/DJAeun/src/optimization_config.py)
- [평가 스크립트](file:///C:/Workspaces/SKN22-3rd-1Team/DJAeun/evaluate_single.py)

---

## 📈 점수 해석 기준

| 점수 범위 | 평가 | 의미 |
|-----------|------|------|
| 0.8 ~ 1.0 | 우수 | 매우 잘 작동, 유지 |
| 0.6 ~ 0.8 | 양호 | 적절히 작동, 개선 가능 |
| 0.4 ~ 0.6 | 보통 | 개선 권장 |
| 0.0 ~ 0.4 | 미흡 | 즉시 개선 필요 |

**Baseline 평균 0.612**: 보통~양호 수준, Answer Relevancy가 발목을 잡고 있음
