"""
ì—¬ëŸ¬ ìµœì í™” ë²„ì „ì„ ì¼ê´„ ë¹„êµ í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import pandas as pd

from tqdm import tqdm
from colorama import Fore, Style, init
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.chain.optimized_rag_chain import prepare_context, generate_answer
from src.config import validate_env
from src.optimization_config import ALL_CONFIGS

# Colorama ì´ˆê¸°í™”
init(autoreset=True)


def print_header(text: str):
    """í—¤ë” ì¶œë ¥"""
    print(f"\n{Fore.CYAN}{'='*70}")
    print(f"{Fore.CYAN}{text:^70}")
    print(f"{Fore.CYAN}{'='*70}{Style.RESET_ALL}\n")


def print_progress(text: str):
    """ì§„í–‰ ìƒíƒœ ì¶œë ¥"""
    print(f"{Fore.YELLOW}â–¶ {text}{Style.RESET_ALL}")


def print_success(text: str):
    """ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥"""
    print(f"{Fore.GREEN}âœ“ {text}{Style.RESET_ALL}")


def print_error(text: str):
    """ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥"""
    print(f"{Fore.RED}âœ— {text}{Style.RESET_ALL}")


def load_test_dataset(filepath: str) -> List[Dict]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def generate_rag_responses_for_config(test_data: List[Dict], config) -> List[Dict]:
    """íŠ¹ì • ì„¤ì •ìœ¼ë¡œ RAG ë‹µë³€ ìƒì„±"""
    results = []
    
    desc = f"{config.name} ë‹µë³€ ìƒì„±"
    for item in tqdm(test_data, desc=desc, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'):
        question = item['question']
        
        try:
            context_data = prepare_context(question, config)
            answer = generate_answer(context_data, config)
            
            result = {
                'question': question,
                'answer': answer,
                'contexts': [context_data['context']],
                'ground_truth': item['ground_truth'],
            }
            results.append(result)
            
        except Exception as e:
            print_error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {question[:30]}... - {str(e)}")
            results.append({
                'question': question,
                'answer': "ë‹µë³€ ìƒì„± ì‹¤íŒ¨",
                'contexts': [""],
                'ground_truth': item['ground_truth'],
            })
    
    return results


def evaluate_config(results: List[Dict], config_name: str) -> Dict:
    """íŠ¹ì • ì„¤ì •ì˜ í‰ê°€"""
    dataset = Dataset.from_list(results)
    
    # í‰ê°€ìš© LLM ë° Embeddings ì„¤ì • (Ragas í˜¸í™˜ì„±)
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    
    eval_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    eval_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ]
    
    try:
        eval_result = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=eval_llm,
            embeddings=eval_embeddings,
        )
        
        # EvaluationResultì—ì„œ ì ìˆ˜ ì¶”ì¶œ - 50ê°œ ì „ì²´ í‰ê·  ê³„ì‚°
        try:
            df = eval_result.to_pandas()
            result_dict = {}
            for col in df.columns:
                if col not in ['user_input', 'retrieved_contexts', 'response', 'reference']:
                    result_dict[col] = df[col].mean()
        except Exception as e:
            print_error(f"ì ìˆ˜ ìµ¡4ì¶œ ì‹¤íŒ¨: {e}")
            result_dict = {}
            for metric in metrics:
                metric_name = metric.name
                if hasattr(eval_result, metric_name):
                    result_dict[metric_name] = getattr(eval_result, metric_name)
        
        return result_dict
    except Exception as e:
        print_error(f"{config_name} í‰ê°€ ì‹¤íŒ¨: {str(e)}")
        return {}


def compare_results(all_results: Dict[str, Dict]):
    """ëª¨ë“  ì„¤ì •ì˜ ê²°ê³¼ ë¹„êµ ì¶œë ¥"""
    print_header("ğŸ“Š ì „ì²´ ë¹„êµ ê²°ê³¼")
    
    # DataFrame ìƒì„±
    df_data = []
    for config_name, metrics in all_results.items():
        if metrics:
            row = {'ì„¤ì •': config_name}
            row.update(metrics)
            df_data.append(row)
    
    if not df_data:
        print_error("ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df = pd.DataFrame(df_data)
    
    # ì„¤ì • ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ
    df = df.set_index('ì„¤ì •')
    
    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
    df['í‰ê· '] = df[numeric_cols].mean(axis=1)
    
    # ì •ë ¬ (í‰ê·  ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
    df = df.sort_values('í‰ê· ', ascending=False)
    
    print(f"{Fore.CYAN}ã€ ì„±ëŠ¥ ìˆœìœ„ ã€‘{Style.RESET_ALL}\n")
    print(df.to_string())
    print()
    
    # ìµœê³  ì„±ëŠ¥ ì„¤ì •
    best_config = df.index[0]
    best_score = df.loc[best_config, 'í‰ê· ']
    
    print(f"{Fore.GREEN}ğŸ† ìµœê³  ì„±ëŠ¥: {best_config} (í‰ê· : {best_score:.4f}){Style.RESET_ALL}\n")
    
    # ê°œì„ ìœ¨ ê³„ì‚° (baseline ëŒ€ë¹„)
    if 'baseline' in df.index:
        baseline_score = df.loc['baseline', 'í‰ê· ']
        improvement = ((best_score - baseline_score) / baseline_score) * 100
        print(f"{Fore.MAGENTA}ğŸ“ˆ Baseline ëŒ€ë¹„ ê°œì„ ìœ¨: {improvement:+.2f}%{Style.RESET_ALL}\n")
    
    return df


def save_comparison_results(all_results: Dict[str, Dict], df: pd.DataFrame, output_dir: Path):
    """ë¹„êµ ê²°ê³¼ ì €ì¥"""
    # JSON ì €ì¥
    json_path = output_dir / "comparison_results.json"
    output_data = {
        'timestamp': datetime.now().isoformat(),
        'results': all_results,
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print_success(f"JSON ê²°ê³¼ ì €ì¥: {json_path}")
    
    # CSV ì €ì¥
    csv_path = output_dir / "comparison_results.csv"
    df.to_csv(csv_path, encoding='utf-8-sig')
    print_success(f"CSV ê²°ê³¼ ì €ì¥: {csv_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_header("ğŸ”¬ RAG ìµœì í™” ë²„ì „ ì¼ê´„ ë¹„êµ í‰ê°€")
    
    # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
    try:
        validate_env()
        print_success("í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì™„ë£Œ")
    except Exception as e:
        print_error(f"í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        sys.exit(1)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).parent
    test_dataset_path = base_dir / "test_dataset.json"
    output_dir = base_dir / "evaluation_results"
    output_dir.mkdir(exist_ok=True)
    
    try:
        # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
        print_progress(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ: {test_dataset_path}")
        test_data = load_test_dataset(str(test_dataset_path))
        print_success(f"{len(test_data)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ")
        
        # 2. ê° ì„¤ì •ë³„ë¡œ í‰ê°€
        print_header(f"ğŸ“ {len(ALL_CONFIGS)}ê°œ ì„¤ì • í‰ê°€ ì‹œì‘")
        
        all_results = {}
        
        for i, config in enumerate(ALL_CONFIGS, 1):
            print(f"\n{Fore.MAGENTA}â”â”â” [{i}/{len(ALL_CONFIGS)}] {config} â”â”â”{Style.RESET_ALL}\n")
            
            # ë‹µë³€ ìƒì„±
            print_progress("ë‹µë³€ ìƒì„± ì¤‘...")
            rag_results = generate_rag_responses_for_config(test_data, config)
            print_success(f"{len(rag_results)}ê°œ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # í‰ê°€
            print_progress("Ragas í‰ê°€ ì¤‘...")
            eval_result = evaluate_config(rag_results, config.name)
            
            if eval_result:
                all_results[config.name] = eval_result
                print_success("í‰ê°€ ì™„ë£Œ")
                
                # ê°„ë‹¨í•œ ê²°ê³¼ ì¶œë ¥
                if 'faithfulness' in eval_result:
                    print(f"  Faithfulness: {eval_result['faithfulness']:.4f}")
                if 'answer_relevancy' in eval_result:
                    print(f"  Answer Relevancy: {eval_result['answer_relevancy']:.4f}")
        
        # 3. ë¹„êµ ê²°ê³¼ ì¶œë ¥
        if all_results:
            df = compare_results(all_results)
            
            # 4. ê²°ê³¼ ì €ì¥
            save_comparison_results(all_results, df, output_dir)
        
        print_header("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ")
        
    except FileNotFoundError as e:
        print_error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)
    except Exception as e:
        print_error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
