"""
í‰ê°€ë§Œ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ë¯¸ë¦¬ ìƒì„±ëœ ë‹µë³€ íŒŒì¼(JSON)ì„ ë¡œë“œí•˜ì—¬ Ragas í‰ê°€ë§Œ ìˆ˜í–‰
"""
import json
import sys
from pathlib import Path
from typing import Dict, List

from colorama import Fore, Style, init
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from src.config import validate_env

init(autoreset=True)


def print_header(text: str):
    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}{text:^60}")
    print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}\n")


def print_progress(text: str):
    print(f"{Fore.YELLOW}â–¶ {text}{Style.RESET_ALL}")


def print_success(text: str):
    print(f"{Fore.GREEN}âœ“ {text}{Style.RESET_ALL}")


def print_error(text: str):
    print(f"{Fore.RED}âœ— {text}{Style.RESET_ALL}")


def print_metric(name: str, value: float):
    if value >= 0.8:
        color = Fore.GREEN
    elif value >= 0.6:
        color = Fore.YELLOW
    else:
        color = Fore.RED
    
    bar_length = int(value * 40)
    bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
    print(f"{name:20s} {color}{bar}{Style.RESET_ALL} {value:.4f}")


def main():
    print_header("ğŸ”¬ Ragas í‰ê°€ ì „ìš© ìŠ¤í¬ë¦½íŠ¸")
    
    # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
    try:
        validate_env()
        print_success("í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì™„ë£Œ")
    except Exception as e:
        print_error(f"í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        sys.exit(1)
    
    # ë‹µë³€ íŒŒì¼ ì°¾ê¸°
    base_dir = Path(__file__).parent
    answers_file = base_dir / "generated_answers.json"
    
    if not answers_file.exists():
        print_error(f"ë‹µë³€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {answers_file}")
        print("\nğŸ’¡ ë‹µë³€ íŒŒì¼ì„ ë¨¼ì € ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤:")
        print("   python evaluate_single.py --config baseline")
        print("   (ì´ì œ ìë™ìœ¼ë¡œ generated_answers.json íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤)")
        sys.exit(1)
    
    # ë‹µë³€ ë¡œë“œ
    print_progress(f"ë‹µë³€ íŒŒì¼ ë¡œë“œ ì¤‘: {answers_file}")
    with open(answers_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    results = data['results']
    config_name = data.get('config', 'unknown')
    
    print_success(f"{len(results)}ê°œ ë‹µë³€ ë¡œë“œ ì™„ë£Œ (ì„¤ì •: {config_name})")
    
    # Ragas í‰ê°€
    print_progress("Ragas í‰ê°€ ì¤€ë¹„ ì¤‘...")
    
    dataset = Dataset.from_list(results)
    
    eval_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    eval_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ]
    
    print(f"\n{Fore.CYAN}í‰ê°€ ì§€í‘œ:{Style.RESET_ALL}")
    print("  â€¢ Faithfulness (ì¶©ì‹¤ë„): ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ê°€")
    print("  â€¢ Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±): ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ê°€")
    print("  â€¢ Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„): ê²€ìƒ‰ì´ ì •í™•í•œê°€")
    print("  â€¢ Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨): í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ ê²€ìƒ‰í–ˆëŠ”ê°€")
    print()
    
    print_progress("í‰ê°€ ì§„í–‰ ì¤‘ (ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    
    try:
        eval_result = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=eval_llm,
            embeddings=eval_embeddings,
        )
        print_success("í‰ê°€ ì™„ë£Œ!")
        
        # EvaluationResultì—ì„œ ì ìˆ˜ ì¶”ì¶œ - 50ê°œ ì „ì²´ í‰ê·  ê³„ì‚°
        try:
            df = eval_result.to_pandas()
            # ê° metric ì—´ì˜ í‰ê· ì„ ê³„ì‚°
            metrics_dict = {}
            for col in df.columns:
                if col not in ['user_input', 'retrieved_contexts', 'response', 'reference']:
                    metrics_dict[col] = df[col].mean()
        except Exception as e:
            print_error(f"ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            metrics_dict = {}
            for metric in metrics:
                metric_name = metric.name
                if hasattr(eval_result, metric_name):
                    metrics_dict[metric_name] = getattr(eval_result, metric_name)
        
        # ê²°ê³¼ ì¶œë ¥
        print_header(f"ğŸ“Š í‰ê°€ ê²°ê³¼ - {config_name}")
        
        print(f"{Fore.CYAN}ã€ í‰ê°€ ì§€í‘œë³„ ì ìˆ˜ ã€‘{Style.RESET_ALL}\n")
        
        for metric_name, score in metrics_dict.items():
            if isinstance(score, (int, float)):
                print_metric(metric_name, score)
        
        numeric_scores = [v for v in metrics_dict.values() if isinstance(v, (int, float))]
        if numeric_scores:
            avg_score = sum(numeric_scores) / len(numeric_scores)
            print(f"\n{Fore.MAGENTA}{'â”€'*60}{Style.RESET_ALL}")
            print_metric("ì „ì²´ í‰ê·  ì ìˆ˜", avg_score)
            print(f"{Fore.MAGENTA}{'â”€'*60}{Style.RESET_ALL}\n")
        
        # ê²°ê³¼ ì €ì¥
        output_file = base_dir / f"evaluation_{config_name}.json"
        output_data = {
            'config': config_name,
            'metrics': {k: float(v) if isinstance(v, (int, float)) else str(v) 
                       for k, v in metrics_dict.items()},
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print_success(f"ê²°ê³¼ ì €ì¥: {output_file}")
        
        print_header("âœ… í‰ê°€ ì™„ë£Œ")
        
    except Exception as e:
        print_error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
