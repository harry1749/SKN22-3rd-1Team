"""
RAG ì‹œìŠ¤í…œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
Ragas ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì˜ì•½í’ˆ ì •ë³´ Q&A ì‹œìŠ¤í…œ í‰ê°€
"""
import json
import sys
import asyncio
from datetime import datetime
from typing import Dict, List
from pathlib import Path

# ì§„í–‰ ìƒíƒœ í‘œì‹œë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°
from tqdm import tqdm
from colorama import Fore, Style, init

# Ragas imports
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.chain.rag_chain import prepare_context, generate_answer
from src.config import validate_env

# Colorama ì´ˆê¸°í™” (Windows í˜¸í™˜)
init(autoreset=True)

def print_header(text: str):
    """í—¤ë” ì¶œë ¥"""
    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}{text:^60}")
    print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}\n")

def print_progress(text: str):
    """ì§„í–‰ ìƒíƒœ ì¶œë ¥"""
    print(f"{Fore.YELLOW}â–¶ {text}{Style.RESET_ALL}")

def print_success(text: str):
    """ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥"""
    print(f"{Fore.GREEN}âœ“ {text}{Style.RESET_ALL}")

def print_error(text: str):
    """ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥"""
    print(f"{Fore.RED}âœ— {text}{Style.RESET_ALL}")

def print_metric(name: str, value: float):
    """í‰ê°€ ì§€í‘œ ì¶œë ¥"""
    # ì ìˆ˜ì— ë”°ë¼ ìƒ‰ìƒ ë³€ê²½
    if value >= 0.8:
        color = Fore.GREEN
    elif value >= 0.6:
        color = Fore.YELLOW
    else:
        color = Fore.RED
    
    bar_length = int(value * 40)
    bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
    print(f"{name:20s} {color}{bar}{Style.RESET_ALL} {value:.4f}")

def load_test_dataset(filepath: str) -> List[Dict]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ"""
    print_progress(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {filepath}")
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print_success(f"ì´ {len(data)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
    return data

def generate_rag_responses(test_data: List[Dict]) -> List[Dict]:
    """RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    print_progress("RAG ì‹œìŠ¤í…œ ë‹µë³€ ìƒì„± ì¤‘...")
    
    results = []
    
    # tqdmì„ ì‚¬ìš©í•œ ì§„í–‰ í‘œì‹œ
    for item in tqdm(test_data, desc="ë‹µë³€ ìƒì„±", bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'):
        question = item['question']
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ë° ë‹µë³€ ìƒì„±
            context_data = prepare_context(question)
            answer = generate_answer(context_data)
            
            # Ragas í‰ê°€ì— í•„ìš”í•œ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
            result = {
                'question': question,
                'answer': answer,
                'contexts': [context_data['context']],  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ
                'ground_truth': item['ground_truth'],
            }
            results.append(result)
            
        except Exception as e:
            print_error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {question[:50]}... - {str(e)}")
            # ì‹¤íŒ¨í•´ë„ ë¹ˆ ë‹µë³€ìœ¼ë¡œ ì¶”ê°€
            results.append({
                'question': question,
                'answer': "ë‹µë³€ ìƒì„± ì‹¤íŒ¨",
                'contexts': [""],
                'ground_truth': item['ground_truth'],
            })
    
    print_success(f"{len(results)}ê°œ ë‹µë³€ ìƒì„± ì™„ë£Œ")
    return results

def evaluate_rag_system(results: List[Dict]) -> Dict:
    """Ragasë¥¼ ì‚¬ìš©í•œ RAG ì‹œìŠ¤í…œ í‰ê°€"""
    print_progress("Ragas í‰ê°€ ì‹œì‘...")
    
    # Dataset ìƒì„±
    dataset = Dataset.from_list(results)
    
    # í‰ê°€ìš© LLM ë° Embeddings ì„¤ì • (Ragas í˜¸í™˜ì„±)
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    
    eval_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    eval_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # í‰ê°€ ì§€í‘œ ì •ì˜
    metrics = [
        faithfulness,           # ì¶©ì‹¤ë„: ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ëŠ”ê°€
        answer_relevancy,       # ë‹µë³€ ê´€ë ¨ì„±: ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ê°€
        context_precision,      # ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì •í™•í•œê°€
        context_recall,         # ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨: í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ ê²€ìƒ‰í–ˆëŠ”ê°€
    ]
    
    print(f"\n{Fore.CYAN}í‰ê°€ ì§€í‘œ:{Style.RESET_ALL}")
    print("  â€¢ Faithfulness (ì¶©ì‹¤ë„): ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ê°€")
    print("  â€¢ Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±): ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ê°€")
    print("  â€¢ Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„): ê²€ìƒ‰ì´ ì •í™•í•œê°€")
    print("  â€¢ Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨): í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ ê²€ìƒ‰í–ˆëŠ”ê°€")
    print()
    
    # í‰ê°€ ì‹¤í–‰ (ì§„í–‰ í‘œì‹œ í¬í•¨) - LLMê³¼ embeddings ëª…ì‹œì  ì „ë‹¬
    print_progress("í‰ê°€ ì§„í–‰ ì¤‘ (ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    
    try:
        eval_result = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=eval_llm,
            embeddings=eval_embeddings,
        )
        print_success("í‰ê°€ ì™„ë£Œ!")
        
        # EvaluationResultì—ì„œ ì ìˆ˜ ì¶”ì¶œ - 50ê°œ ì „ì²´ í‰ê·  ê³„ì‚°
        try:
            df = eval_result.to_pandas()
            result_dict = {}
            for col in df.columns:
                if col not in ['question', 'answer', 'contexts', 'ground_truth']: # Ragas 0.1.0 ì´í›„ ë³€ê²½ëœ ì»¬ëŸ¼ëª… ë°˜ì˜
                    result_dict[col] = df[col].mean()
        except Exception as e:
            print_error(f"ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            result_dict = {}
            for metric in metrics:
                metric_name = metric.name
                result_dict[metric_name] = getattr(eval_result, metric_name)
        
        return result_dict
        
    except Exception as e:
        print_error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

def display_results(eval_result: Dict, output_file: str = None):
    """í‰ê°€ ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥"""
    print_header("ğŸ“Š í‰ê°€ ê²°ê³¼")
    
    # ê° ì§€í‘œë³„ ì ìˆ˜ ì¶œë ¥
    metrics_dict = eval_result
    
    print(f"{Fore.CYAN}ã€ í‰ê°€ ì§€í‘œë³„ ì ìˆ˜ ã€‘{Style.RESET_ALL}\n")
    
    for metric_name, score in metrics_dict.items():
        if isinstance(score, (int, float)):
            print_metric(metric_name, score)
    
    # ì „ì²´ í‰ê·  ì ìˆ˜ ê³„ì‚°
    numeric_scores = [v for v in metrics_dict.values() if isinstance(v, (int, float))]
    if numeric_scores:
        avg_score = sum(numeric_scores) / len(numeric_scores)
        print(f"\n{Fore.MAGENTA}{'â”€'*60}{Style.RESET_ALL}")
        print_metric("ì „ì²´ í‰ê·  ì ìˆ˜", avg_score)
        print(f"{Fore.MAGENTA}{'â”€'*60}{Style.RESET_ALL}\n")
        
        # ì ìˆ˜ í•´ì„
        if avg_score >= 0.8:
            assessment = f"{Fore.GREEN}ìš°ìˆ˜{Style.RESET_ALL} - RAG ì‹œìŠ¤í…œì´ ë§¤ìš° ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!"
        elif avg_score >= 0.6:
            assessment = f"{Fore.YELLOW}ì–‘í˜¸{Style.RESET_ALL} - ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆì§€ë§Œ ì ì ˆí•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤."
        else:
            assessment = f"{Fore.RED}ê°œì„  í•„ìš”{Style.RESET_ALL} - ì‹œìŠ¤í…œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        print(f"ì¢…í•© í‰ê°€: {assessment}\n")
    
    # ê²°ê³¼ ì €ì¥
    if output_file:
        save_results(metrics_dict, output_file)

def save_results(results: Dict, output_file: str):
    """í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_data = {
        'timestamp': datetime.now().isoformat(),
        'metrics': {k: float(v) if isinstance(v, (int, float)) else str(v) 
                   for k, v in results.items()},
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print_success(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_header("ğŸ”¬ FDA ì˜ì•½í’ˆ ì •ë³´ RAG ì‹œìŠ¤í…œ í‰ê°€")
    
    # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
    try:
        validate_env()
        print_success("í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì™„ë£Œ")
    except Exception as e:
        print_error(f"í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        sys.exit(1)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    test_dataset_path = Path(__file__).parent / "test_dataset.json"
    output_path = Path(__file__).parent / "evaluation_results.json"
    
    try:
        # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
        test_data = load_test_dataset(str(test_dataset_path))
        
        # 2. RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
        rag_results = generate_rag_responses(test_data)
        
        # 3. Ragasë¡œ í‰ê°€
        eval_result = evaluate_rag_system(rag_results)
        
        # 4. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
        display_results(eval_result, str(output_path))
        
        print_header("âœ… í‰ê°€ ì™„ë£Œ")
        
    except FileNotFoundError as e:
        print_error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)
    except Exception as e:
        print_error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
