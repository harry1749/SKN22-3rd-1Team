"""
ë‹¨ì¼ ìµœì í™” ë²„ì „ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
íŠ¹ì • ì„¤ì •ìœ¼ë¡œë§Œ í‰ê°€ë¥¼ ì‹¤í–‰
"""
import json
import sys
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List

from tqdm import tqdm
from colorama import Fore, Style, init
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

from src.chain.optimized_rag_chain import prepare_context, generate_answer
from src.config import validate_env
from src.optimization_config import get_config, ALL_CONFIGS

init(autoreset=True)


def print_header(text: str):
    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}{text:^60}")
    print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}\n")


def print_progress(text: str):
    print(f"{Fore.YELLOW}â–¶ {text}{Style.RESET_ALL}")


def print_success(text: str):
    print(f"{Fore.GREEN}âœ“ {text}{Style.RESET_ALL}")


def print_error(text: str):
    print(f"{Fore.RED}âœ— {text}{Style.RESET_ALL}")


def print_metric(name: str, value: float):
    if value >= 0.8:
        color = Fore.GREEN
    elif value >= 0.6:
        color = Fore.YELLOW
    else:
        color = Fore.RED
    
    bar_length = int(value * 40)
    bar = "â–ˆ" * bar_length + "â–‘" * (40 - bar_length)
    print(f"{name:20s} {color}{bar}{Style.RESET_ALL} {value:.4f}")


def load_test_dataset(filepath: str) -> List[Dict]:
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def generate_rag_responses(test_data: List[Dict], config) -> List[Dict]:
    results = []
    
    for item in tqdm(test_data, desc="ë‹µë³€ ìƒì„±", bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'):
        question = item['question']
        
        try:
            context_data = prepare_context(question, config)
            answer = generate_answer(context_data, config)
            
            result = {
                'question': question,
                'answer': answer,
                'contexts': [context_data['context']],
                'ground_truth': item['ground_truth'],
            }
            results.append(result)
            
        except Exception as e:
            print_error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {question[:50]}... - {str(e)}")
            results.append({
                'question': question,
                'answer': "ë‹µë³€ ìƒì„± ì‹¤íŒ¨",
                'contexts': [""],
                'ground_truth': item['ground_truth'],
            })
    
    # ë‹µë³€ì„ ìë™ìœ¼ë¡œ JSON íŒŒì¼ì— ì €ì¥
    save_path = Path(__file__).parent / "generated_answers.json"
    save_data = {
        'config': config.name,
        'results': results,
    }
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    print_success(f"ë‹µë³€ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    return results


def evaluate_rag_system(results: List[Dict]) -> Dict:
    dataset = Dataset.from_list(results)
    
    # í‰ê°€ìš© LLM ë° Embeddings ì„¤ì • (Ragas í˜¸í™˜ì„±)
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    
    eval_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    eval_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ]
    
    print(f"\n{Fore.CYAN}í‰ê°€ ì§€í‘œ:{Style.RESET_ALL}")
    print("  â€¢ Faithfulness (ì¶©ì‹¤ë„): ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ê°€")
    print("  â€¢ Answer Relevancy (ë‹µë³€ ê´€ë ¨ì„±): ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ê°€")
    print("  â€¢ Context Precision (ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„): ê²€ìƒ‰ì´ ì •í™•í•œê°€")
    print("  â€¢ Context Recall (ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨): í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ ê²€ìƒ‰í–ˆëŠ”ê°€")
    print()
    
    print_progress("í‰ê°€ ì§„í–‰ ì¤‘ (ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    
    try:
        eval_result = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=eval_llm,
            embeddings=eval_embeddings,
        )
        print_success("í‰ê°€ ì™„ë£Œ!")
        
        # EvaluationResultì—ì„œ ì ìˆ˜ ì¶”ì¶œ - 50ê°œ ì „ì²´ í‰ê·  ê³„ì‚°
        try:
            df = eval_result.to_pandas()
            result_dict = {}
            for col in df.columns:
                if col not in ['user_input', 'retrieved_contexts', 'response', 'reference']:
                    result_dict[col] = df[col].mean()
        except Exception as e:
            print_error(f"ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            result_dict = {}
            for metric in metrics:
                metric_name = metric.name
                if hasattr(eval_result, metric_name):
                    result_dict[metric_name] = getattr(eval_result, metric_name)
        
        return result_dict
    except Exception as e:
        print_error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise


def display_results(eval_result: Dict, config_name: str, output_file: str = None):
    print_header(f"ğŸ“Š í‰ê°€ ê²°ê³¼ - {config_name}")
    
    metrics_dict = eval_result
    
    print(f"{Fore.CYAN}ã€ í‰ê°€ ì§€í‘œë³„ ì ìˆ˜ ã€‘{Style.RESET_ALL}\n")
    
    for metric_name, score in metrics_dict.items():
        if isinstance(score, (int, float)):
            print_metric(metric_name, score)
    
    numeric_scores = [v for v in metrics_dict.values() if isinstance(v, (int, float))]
    if numeric_scores:
        avg_score = sum(numeric_scores) / len(numeric_scores)
        print(f"\n{Fore.MAGENTA}{'â”€'*60}{Style.RESET_ALL}")
        print_metric("ì „ì²´ í‰ê·  ì ìˆ˜", avg_score)
        print(f"{Fore.MAGENTA}{'â”€'*60}{Style.RESET_ALL}\n")
        
        if avg_score >= 0.8:
            assessment = f"{Fore.GREEN}ìš°ìˆ˜{Style.RESET_ALL} - RAG ì‹œìŠ¤í…œì´ ë§¤ìš° ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!"
        elif avg_score >= 0.6:
            assessment = f"{Fore.YELLOW}ì–‘í˜¸{Style.RESET_ALL} - ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆì§€ë§Œ ì ì ˆí•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤."
        else:
            assessment = f"{Fore.RED}ê°œì„  í•„ìš”{Style.RESET_ALL} - ì‹œìŠ¤í…œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        print(f"ì¢…í•© í‰ê°€: {assessment}\n")
    
    if output_file:
        save_results(metrics_dict, config_name, output_file)


def save_results(results: Dict, config_name: str, output_file: str):
    output_data = {
        'timestamp': datetime.now().isoformat(),
        'config': config_name,
        'metrics': {k: float(v) if isinstance(v, (int, float)) else str(v) 
                   for k, v in results.items()},
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print_success(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")


def main():
    parser = argparse.ArgumentParser(description='RAG ì‹œìŠ¤í…œ ë‹¨ì¼ ì„¤ì • í‰ê°€')
    parser.add_argument(
        '--config',
        type=str,
        default='baseline',
        help=f'í‰ê°€í•  ì„¤ì • ì´ë¦„ (ê¸°ë³¸ê°’: baseline). ê°€ëŠ¥í•œ ê°’: {", ".join([c.name for c in ALL_CONFIGS])}'
    )
    
    args = parser.parse_args()
    
    print_header(f"ğŸ”¬ RAG ì‹œìŠ¤í…œ í‰ê°€ - {args.config}")
    
    try:
        validate_env()
        print_success("í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì™„ë£Œ")
    except Exception as e:
        print_error(f"í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        sys.exit(1)
    
    # ì„¤ì • ë¡œë“œ
    try:
        config = get_config(args.config)
        print_success(f"ì„¤ì • ë¡œë“œ: {config}")
    except ValueError as e:
        print_error(str(e))
        sys.exit(1)
    
    base_dir = Path(__file__).parent
    test_dataset_path = base_dir / "test_dataset.json"
    output_path = base_dir / f"evaluation_{args.config}.json"
    
    try:
        # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
        test_data = load_test_dataset(str(test_dataset_path))
        print_success(f"{len(test_data)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ")
        
        # 2. RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
        print_progress("RAG ì‹œìŠ¤í…œ ë‹µë³€ ìƒì„± ì¤‘...")
        rag_results = generate_rag_responses(test_data, config)
        
        # 3. Ragasë¡œ í‰ê°€
        eval_result = evaluate_rag_system(rag_results)
        
        # 4. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
        display_results(eval_result, args.config, str(output_path))
        
        print_header("âœ… í‰ê°€ ì™„ë£Œ")
        
    except FileNotFoundError as e:
        print_error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)
    except Exception as e:
        print_error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
